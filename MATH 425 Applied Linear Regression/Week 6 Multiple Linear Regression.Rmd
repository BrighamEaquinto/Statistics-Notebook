---
title: "Week 6 Multiple Linear Regression"
author: "Brigham Eaquinto"
date: "10/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(pander)
```

## Class Activity - Expanding the Regression Model


### Quadratic Regression Model

```{r}
  ## Simulating Data from a Regression Model
  ## This R-chunk is meant to be played in your R Console.
  ## It allows you to explore how the various elements
  ## of the regression model combine together to "create"
  ## data and then use the data to "re-create" the line.

  set.seed(101) #Allows us to always get the same "random" sample
                #Change to a new number to get a new sample

  n <- 100 #set the sample size

  X_i <- runif(n, -15, 10) 
    #Gives n random values from a uniform distribution between 15 to 45.

  beta0 <- 0 #Our choice for the y-intercept. 

  beta1 <- 0 #Our choice for the slope. 
  
  beta2 <- -0.05 # this is what we're playing with

  sigma <- 0.5 #Our choice for the std. deviation of the error terms. Sigma is the average variation from the law


  epsilon_i <- rnorm(n, 0, sigma) 
    #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

  Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + epsilon_i 
    #Create Y using the normal error regression model

  fabData <- data.frame(y=Y_i, x=X_i) 
    #Store the data as data

  # View(fabData)
  

  #In the real world, we begin with data (like fabData) and try to recover the model that 
  # (we assume) was used to created it.

  fab.lm <- lm(y ~ x+ I(x^2), data=fabData) #Fit an estimated regression model to the fabData.
  
  b <- coef(fab.lm)

  summary(fab.lm) #Summarize your model. 

  plot(y ~ x, data=fabData) #Plot the data.
  curve(b[1] + b[2]*x + b[3]*x^2, add = TRUE)

  
  # abline(fab.lm) #Add the estimated regression line to your plot.

  # Now for something you can't do in real life... but since we created the data...

  # abline(beta0, beta1, lty=2) 
    #Add the true regression line to your plot using a dashed line (lty=2). 

  # legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n") 
    #Add a legend to your plot specifying which line is which.
```


<br>


## Class Activity - Different Types of Models

From today's lecture we went over Multiple Linear Regression: Two_Lines, Quadratic, and Cubic. They all make different lines. These are the building blocks of multiple linear regression. Similar to that idea from the week on **transformations** where different clusters of observations had different transformation laws to them, there can be multiple-different types equations combined together to create **MADNESS**!


Sigma is the average variation from the law

<br>

### Cubic Regression Model

```{r}
  n <- 100 #set the sample size

  X_i <- runif(n, -4, 4) 
    #Gives n random values from a uniform distribution between 15 to 45.

  beta0 <- 0 #Our choice for the y-intercept. 

  beta1 <- -10 #Our choice for the slope. 
  
  beta2 <- 0 # this is what we're playing with
  
  beta3 <- 1

  sigma <- 3 #Our choice for the std. deviation of the error terms. Sigma is the average variation from the law

  epsilon_i <- rnorm(n, 0, sigma) 
    #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

  Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + beta3*X_i^3 + epsilon_i 
    #Create Y using the normal error regression model

  fabData <- data.frame(y=Y_i, x=X_i) 
    #Store the data as data

  # View(fabData)

  #In the real world, we begin with data (like fabData) and try to recover the model that 
  # (we assume) was used to created it.

  fab.lm <- lm(y ~ x+ I(x^2) + I(x^3), data=fabData) #Fit an estimated regression model to the fabData.
  
  b <- coef(fab.lm)

  summary(fab.lm) #Summarize your model. 

  plot(y ~ x, data=fabData) #Plot the data.
  curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add = TRUE)

```

<br>


### Two-Lines Regression Model

```{r}

n <- 100 #set the sample size

  X_1i <- runif(n, -8, 88) #Gives n random values from a uniform distribution between 15 to 45.
  X_2i <- sample(c(0,1), n, replace = TRUE)
  beta0 <- 3 #Our choice for the y-intercept. 
  beta1 <- -10 #Our choice for the slope. 
  beta2 <- -400 # this is what we're playing with
  beta3 <- 20
  sigma <- 80 #Our choice for the std. deviation of the error terms. Sigma is the average variation from the law
  epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
  Y_i <- beta0 + beta1*X_1i + beta2*X_2i + beta3*X_1i*X_2i + epsilon_i 
    #Create Y using the normal error regression model
  fabData <- data.frame(y=Y_i, x=X_1i, x2 = X_2i) 
    #Store the data as data

  # View(fabData)

  #In the real world, we begin with data (like fabData) and try to recover the model that 
  # (we assume) was used to created it.

  lm.2lines <- lm(y ~ x + x2 + x:x2, data=fabData) #Fit an estimated regression model to the fabData. 
  b <- coef(lm.2lines)
  summary(lm.2lines) #Summarize your model. 
  
  plot(y ~ x, col = as.factor(x2), data=fabData) #Plot the data. 
  
  x2=0 
  curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE) 

  x2=1
  curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col = 'red') 
  


```


## Skills Quiz (Mobius) - Different Types of Models

### Question 1

#### Part A

```{r Part A} 
library(mosaic)
?Utilities
str(Utilities)


mylm <- lm(gasbill ~ month + I(month^2), data = Utilities)
summary(mylm)
b <- coef(mylm)
plot(gasbill ~ month, data = Utilities)
curve(b[1] + b[2]*x + b[3]*x^2, add = TRUE)


```
#### Part B
```{r Part B}

par(mfrow=c(1,3))
plot(mylm,which=1:2)
plot(mylm$residuals)

```
#### Part C
```{r Part C}

predict(mylm, data.frame(month=9), interval="confidence")

```
#### Part D
```{r Part D}

Utilities %>% 
  group_by(month) %>% 
  summarise(mean(gasbill))

```



#### Part E

- ***MSE is residual standard error squared***
- **Residual Standard Error**: Gives an estimate of the standard deviation of the actual gas bill values around the model's average gas bill amount for each month.
- **MSE**: The estimate of the model variance  Ïƒ^2
- **R^2**: The measure of the proportion of variability in gas bill prices explained by the regression model.

```{r Part E}
# mse is residual standard error squared
sqrt(30.48)

```


### Question 2

```{r}
View(mtcars)
?mtcars

mylm <- lm(mpg ~ qsec + am + qsec:am, data = mtcars)
summary(mylm)


plot(mpg ~ qsec, data=mtcars, col=c("skyblue","orange")[as.factor(am)], pch=21, bg="gray83", main="Two-lines Model using mtcars data set", cex.main=1)

legend("topleft", legend=c("Baseline (am==0)", "Changed-line (am==1)"), bty="n", lty=1, col=c("skyblue","orange"), cex=0.8)

#get the "Estimates" automatically:
b <- coef(mylm)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -9.0099
# b[2] is the estimate of beta_1:  1.4385
# b[3] is the estimate of beta_2: -14.5107
# b[4] is the estimate of beta_3: 1.3214
curve(b[1] + b[2]*x, col="skyblue", lwd=2, add=TRUE)  #baseline (in blue)
curve((b[1] + b[3]) + (b[2] + b[4])*x, col="orange", lwd=2, add=TRUE) #changed line (in orange)


```

#### Part D
```{r Part D}
par(mfrow=c(1,3))
plot(mylm,which=1:2)
plot(mylm$residuals)

```


### Question 3

```{r}
mylm <- lm(qsec ~ disp + I(disp^2) + am + disp:am + I(disp^2):am, data = mtcars)
summary(mylm)
b <- coef(mylm) 

plot(qsec ~ disp, data=mtcars, col=c("skyblue","orange")[as.factor(am)], pch=21, bg="gray83", main="Quarter Second Mile Predicted by \nEngine Displacement and Transmission", cex.main=1)

x2=0
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x2 + b[5]*x*x2 + b[6]*x^2*x2, col="skyblue", lwd=2, add=TRUE)  #baseline (in blue)
x2=1
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x2 + b[5]*x*x2 + b[6]*x^2*x2, col="orange", lwd=2, add=TRUE) #changed line (in orange)
```


```{r}
par(mfrow=c(1,3))
plot(mylm,which=1:2)
plot(mylm$residuals)
```





## Class Activity - Reviewing the Different Models

```{r}
challData <- data.frame(y=c( 62.0230141814838,65.3287607440202,63.0850996714816,64.7215795301002,63.5747270663006,53.702998209753,67.3864948748882,63.6127706891815,67.7168411573423,65.1905587130745,62.7203672543605,65.9786946907104,65.9747299777148,66.0078406427285,56.6621327894133,64.427226260792,59.6411522580869,54.7853006620191,62.5791157289376,64.2432705505488,64.6637314187692,66.0977643647167,66.1309505076381,63.2605451217536,65.8227827462007,65.2274912077561,67.8282521078083,65.7658686316158,62.295434407344,57.8409797705986 ),  x=c( 2.0260852817446,9.03627189621329,3.7256769053638,10.362243656069,11.1665419801138,-1.36220900854096,5.39347683265805,10.4938666215166,5.72009020252153,4.39260629424825,11.3956668348983,4.34667818667367,7.48598889634013,6.01686762738973,-0.559054442681372,10.5975495856255,1.44522828096524,-1.41116653056815,2.59089006995782,11.3630510880612,10.4535504248925,7.69924768619239,6.96709539275616,11.9197768727317,7.1798811876215,7.91942655434832,5.61692434595898,6.31798828626052,2.04823632212356,0.0595910623669624 ), x2=c( 1,0,1,0,0,1,1,0,1,1,0,1,0,0,1,0,1,1,1,0,0,0,0,0,0,0,1,0,1,1 ))

# View(challData)

pairs(challData)
```



```{r}
# this is how to decide which model would be guess! Intro, more to come later

# quadratic parabola
lm.quad <- lm(y ~ x + I(x^2), data = challData)
summary(lm.quad)
plot(lm.quad, which = 1)

# cubic
cube.lm <- lm(y ~ x + I(x^2) + I(x^3), data = challData)
summary(cube.lm) # this shows pvalue not significant
plot(cube.lm, which = 1)

# two lines
twolines.lm <- lm(y ~ x + x2, + x:x2, data = challData)
summary(twolines.lm)
plot(twolines.lm, which = 1)


# R^2 = 1 - (SSE/SSTO)
# Anytime there is a more complex model, r^2 (multiple r squared) will be better. It is because of overfitting. 
# Adjusted R-squared is always lower than multiple r squared. 
# 


pairs(challData, panel = panel.smooth)

```




