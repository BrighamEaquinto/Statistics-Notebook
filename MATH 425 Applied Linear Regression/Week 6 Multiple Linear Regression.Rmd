---
title: "Week 6 Multiple Linear Regression"
author: "Brigham Eaquinto"
date: "10/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Monday


### Quadratic Regression Model

```{r}
  ## Simulating Data from a Regression Model
  ## This R-chunk is meant to be played in your R Console.
  ## It allows you to explore how the various elements
  ## of the regression model combine together to "create"
  ## data and then use the data to "re-create" the line.

  set.seed(101) #Allows us to always get the same "random" sample
                #Change to a new number to get a new sample

  n <- 100 #set the sample size

  X_i <- runif(n, -15, 10) 
    #Gives n random values from a uniform distribution between 15 to 45.

  beta0 <- 0 #Our choice for the y-intercept. 

  beta1 <- 0 #Our choice for the slope. 
  
  beta2 <- -0.05 # this is what we're playing with

  sigma <- 0.5 #Our choice for the std. deviation of the error terms. Sigma is the average variation from the law


  epsilon_i <- rnorm(n, 0, sigma) 
    #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

  Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + epsilon_i 
    #Create Y using the normal error regression model

  fabData <- data.frame(y=Y_i, x=X_i) 
    #Store the data as data

  # View(fabData)
  

  #In the real world, we begin with data (like fabData) and try to recover the model that 
  # (we assume) was used to created it.

  fab.lm <- lm(y ~ x+ I(x^2), data=fabData) #Fit an estimated regression model to the fabData.
  
  b <- coef(fab.lm)

  summary(fab.lm) #Summarize your model. 

  plot(y ~ x, data=fabData) #Plot the data.
  curve(b[1] + b[2]*x + b[3]*x^2, add = TRUE)

  
  # abline(fab.lm) #Add the estimated regression line to your plot.

  # Now for something you can't do in real life... but since we created the data...

  # abline(beta0, beta1, lty=2) 
    #Add the true regression line to your plot using a dashed line (lty=2). 

  # legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n") 
    #Add a legend to your plot specifying which line is which.
```


<br>


## Wednesday

From today's lecture we went over Multiple Linear Regression: Two_Lines, Quadratic, and Cubic. They all make different lines. These are the building blocks of multiple linear regression. Similar to that idea from the week on **transformations** where different clusters of observations had different transformation laws to them, there can be multiple-different types equations combined together to create **MADNESS**!


Sigma is the average variation from the law

<br>

### Cubic Regression Model

```{r}
  n <- 100 #set the sample size

  X_i <- runif(n, -4, 4) 
    #Gives n random values from a uniform distribution between 15 to 45.

  beta0 <- 0 #Our choice for the y-intercept. 

  beta1 <- -10 #Our choice for the slope. 
  
  beta2 <- 0 # this is what we're playing with
  
  beta3 <- 1

  sigma <- 3 #Our choice for the std. deviation of the error terms. Sigma is the average variation from the law

  epsilon_i <- rnorm(n, 0, sigma) 
    #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

  Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + beta3*X_i^3 + epsilon_i 
    #Create Y using the normal error regression model

  fabData <- data.frame(y=Y_i, x=X_i) 
    #Store the data as data

  # View(fabData)

  #In the real world, we begin with data (like fabData) and try to recover the model that 
  # (we assume) was used to created it.

  fab.lm <- lm(y ~ x+ I(x^2) + I(x^3), data=fabData) #Fit an estimated regression model to the fabData.
  
  b <- coef(fab.lm)

  summary(fab.lm) #Summarize your model. 

  plot(y ~ x, data=fabData) #Plot the data.
  curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add = TRUE)

```

<br>


### Two-Lines Regression Model

```{r}

n <- 100 #set the sample size

  X_1i <- runif(n, -8, 8) #Gives n random values from a uniform distribution between 15 to 45.
  X_2i <- runif(n, -8, 8) 
  beta0 <- 0 #Our choice for the y-intercept. 
  beta1 <- -10 #Our choice for the slope. 
  beta2 <- 0 # this is what we're playing with
  beta3 <- 1
  sigma <- 3 #Our choice for the std. deviation of the error terms. Sigma is the average variation from the law
  epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
  Y_i <- beta0 + beta1*X_1i + beta2*X_2i + beta3*X_1i*X_2i + epsilon_i 
    #Create Y using the normal error regression model
  fabData <- data.frame(y=Y_i, x=X_i) 
    #Store the data as data

  # View(fabData)

  #In the real world, we begin with data (like fabData) and try to recover the model that 
  # (we assume) was used to created it.

  lm.2lines <- lm(y ~ X_1i + X_2i + X_1i:X_2i, data=fabData) #Fit an estimated regression model to the fabData.
  b <- coef(lm.2lines)
  summary(lm.2lines) #Summarize your model. 
  
  plot(y ~ x, data=fabData) #Plot the data.
  curve(b[1] + b[2]*x + b[3]*x + b[4]*x, add = TRUE)


```


## Skills Quiz (Mobius) - Different Types of Models

### Question 1

#### Part A

```{r Part A} 
library(mosaic)
?Utilities
str(Utilities)


mylm <- lm(gasbill ~ month + I(month^2), data = Utilities)
summary(mylm)
b <- coef(mylm)
plot(gasbill ~ month, data = Utilities)
curve(b[1] + b[2]*x + b[3]*x^2, add = TRUE)


```
#### Part B
```{r Part B}

par(mfrow=c(1,3))
plot(mylm,which=1:2)
plot(mylm$residuals)

```
#### Part C
```{r Part C}

predict(mylm, data.frame(month=9), interval="confidence")

```
#### Part D
```{r Part D}

Utilities %>% 
  group_by(month) %>% 
  summarise(mean(gasbill))

```



#### Part E

- ***MSE is residual standard error squared***
- **Residual Standard Error**: Gives an estimate of the standard deviation of the actual gas bill values around the model's average gas bill amount for each month.
- **MSE**: The estimate of the model variance  Ïƒ^2
- **R^2**: The measure of the proportion of variability in gas bill prices explained by the regression model.

```{r Part E}
# mse is residual standard error squared
sqrt(30.48)

```


### Question 2

```{r}
View(mtcars)
?mtcars

mylm <- lm(mpg ~ qsec + am + qsec:am, data = mtcars)
summary(mylm)


plot(mpg ~ qsec, data=mtcars, col=c("skyblue","orange")[as.factor(am)], pch=21, bg="gray83", main="Two-lines Model using mtcars data set", cex.main=1)

legend("topleft", legend=c("Baseline (am==0)", "Changed-line (am==1)"), bty="n", lty=1, col=c("skyblue","orange"), cex=0.8)

#get the "Estimates" automatically:
b <- coef(mylm)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -9.0099
# b[2] is the estimate of beta_1:  1.4385
# b[3] is the estimate of beta_2: -14.5107
# b[4] is the estimate of beta_3: 1.3214
curve(b[1] + b[2]*x, col="skyblue", lwd=2, add=TRUE)  #baseline (in blue)
curve((b[1] + b[3]) + (b[2] + b[4])*x, col="orange", lwd=2, add=TRUE) #changed line (in orange)


```

#### Part D
```{r Part D}
par(mfrow=c(1,3))
plot(mylm,which=1:2)
plot(mylm$residuals)

```


### Question 3

```{r}
mylm <- lm(qsec ~ disp + am + disp:am, data = mtcars)

summary(mylm)

b <- coef(mylm)

plot(qsec ~ disp, data=mtcars, col=c("skyblue","orange")[as.factor(am)], pch=21, bg="gray83", main="Quarter Second Mile Predicted by Engine Displacement and Transmission", cex.main=1)

curve(b[1] + b[2]*x, col="skyblue", lwd=2, add=TRUE)  #baseline (in blue)

curve((b[1] + b[3]) + (b[2] + b[4])*x, col="orange", lwd=2, add=TRUE) #changed line (in orange)
```


#### Part A


#### Part B


#### Part C


#### Part D


#### Part E

