---
title: "Week 7 Lowess Curves and Model Selection"
author: "Brigham Eaquinto"
date: "10/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Class Activity - Introduction to Model Selection (Monday)

```{r, message=FALSE, warning=FALSE}
p1Data <- read.csv("p1Data-1.csv", header=TRUE)
p2Data <- read.csv("p2Data-1.csv", header=TRUE)
p3Data <- read.csv("p3Data-1.csv", header=TRUE)
```

### Part 1 - Recovering a True Model (Beginner Level)

We believe the true model to be...

$$
  Y_i = \underbrace{\beta_0 + \beta_1 X_{4i} + \beta_2 X_{4i} X}_\text{The True Model} + \epsilon_i
$$



And estimate that model by...

$$
  \hat{Y}_i = -0.7065 +2.3958 X_{4i} + 1.7763 X_{4i} X_{2i}
$$

with our estimate of $\sigma$ as 13.11.

```{r}

pairs(p1Data)
pairs(p1Data, panel = panel.smooth, col = as.factor(p1Data$X2))

mylm1 <- lm(Y~X4, data = p1Data)
summary(mylm1)

plot(mylm1$residuals ~ ., data = p1Data, col = as.factor(p1Data$X2))
# this period is like SQL's *. It selects all columns one at a time in this case)

mylm2 <- lm(Y~X4 + X2 + X4:X2, data = p1Data)
summary(mylm2)




mylm1 <- lm(Y~X2, data = p1Data)
summary(mylm1)

plot(mylm1$residuals ~ ., data = p1Data, col = as.factor(p1Data$X2))
# this period is like SQL's *. It selects all columns one at a time in this case)

mylm2 <- lm(Y~X4 + X2, data = p1Data)
summary(mylm2)


# 
pairs(cbind(Res = mylm2$residuals, fit = mylm2$fitted.values, p1Data), panel=panel.smooth, col = as.factor(p1Data$X2))

mylm3 <- lm(Y~X4 + X2 + X3, data = p1Data)
summary(mylm3)
# remember, is a variable is not usless is it's not significant. Usefulness at this point in the model at predicting Y. 


mylm4 <- lm(Y~X4 + X2 + X4:X2, data = p1Data)
summary(mylm4)
# remember, interactions are what let things tilt/ 


mylm5 <- lm(Y~X4 + X4:X2, data = p1Data)
summary(mylm5)

```



```{r}
pairs(p1Data)
pairs(p1Data, panel = panel.smooth, col = as.factor(p1Data$X2))

mylm1 <- lm(Y~X4, data=p1Data)
summary(mylm1)

plot(mylm1$residuals~ ., data = p1Data) #The dot iterates along each column of p1Data

mylm2 <- lm(Y~X4 + X2, data=p1Data)
summary(mylm2)


# mylm1 <- lm(Y~X2, data=p1Data)
# summary(mylm1)

plot(mylm1$residuals~ ., data = p1Data) #The dot iterates along each column of p1Data

mylm2 <- lm(Y~X4 + X2, data=p1Data)
summary(mylm2)

pairs(cbind(Res = mylm2$residuals, fit = mylm2$fitted.values, p1Data), panel=panel.smooth, col = as.factor(p1Data$X2))

mylm3 <- lm(Y~X4 + X2+ X3, data=p1Data) #X3 is not significant (yet)
summary(mylm3)

mylm4 <- lm(Y~X4 + X2 + X4:X2, data=p1Data)
summary(mylm4)

mylm5 <- lm(Y~X4 +X4:X2, data=p1Data)
summary(mylm5)
```


### Part 2 - Recovering a True Model (Intermediate Level)

We believe the true model to be...

$$
  Y_i = \underbrace{...}_\text{The True Model} + \epsilon_i
$$

And estimate that model by...

$$
  \hat{Y}_i = ...
$$

with our estimate of $\sigma$ as ...

```{r}
pairs(p2Data, panel = panel.smooth)

mylm1 <- lm(Y~ X5, data = p2Data)
summary(mylm1)

plot(mylm1$residuals ~., data = p2Data)


mylm2 <- lm(Y~ X5 + I(X5^2) + X3, data = p2Data)
summary(mylm2)


```

```{r}
#install.packages("plotly")
library(plotly)
#install.packages("reshape2")
library(reshape2)
#Perform the multiple regression
air_lm <- lm(Y~ X5 + I(X5^2) + X3, data = p2Data)

#Graph Resolution (more important for more complex shapes)
graph_reso <- 0.5

#Setup Axis
axis_x <- seq(min(p2Data$X5), max(p2Data$X5), by = graph_reso)
axis_y <- seq(min(p2Data$X3), max(p2Data$X3), by = graph_reso)

#Sample points
air_surface <- expand.grid(X5 = axis_x, X3 = axis_y, KEEP.OUT.ATTRS=F)
air_surface <- air_surface %>% mutate(Z=predict.lm(air_lm, newdata = air_surface))
air_surface <- acast(air_surface, X3 ~ X5, value.var = "Z") #y ~ x

#Create scatterplot
plot_ly(p2Data, 
        x = ~X5, 
        y = ~X3, 
        z = ~Y,
        text = rownames(p2Data), 
        type = "scatter3d", 
        mode = "markers") %>%
  add_trace(z = air_surface,
            x = axis_x,
            y = axis_y,
            type = "surface")
```


### Part 3 - Recovering a True Model (Advanced Level)

We believe the true model to be...

$$
  Y_i = \underbrace{...}_\text{The True Model} + \epsilon_i
$$

And estimate that model by...

$$
  \hat{Y}_i = ...
$$

with our estimate of $\sigma$ as ...

```{r}
pairs(p3Data)
```










[Link to specific]()

## Class Activity - Introduction to Lowess Curves (Wednesday)

```{r}
# Part 1 Weighted Regressions 
# We're going to be learning about giving different weights to variables

X <- c(1, 4,  7,   8,  10, 20)

Y <- c(3, 5, 18, 13, 12,   1)

w <- c(1, .5, .2, 1, 1, .1)
w <- c(1, 1, 0, 0, 0, 0, 0)
w <- c(0, 0, 1, 0, 0, 1)
w <- c(1, 1, 0.5, 1, 1, 0.2)

mylm <- lm(Y ~ X, weights=w)
summary(mylm)

plot(Y ~ X, pch=21, bg=rgb(1-w,1-w,1-w), col="orange")

abline(mylm)


# Systematic way of trusting data: Lowess Curve
# Lowess: Local weighted scatterplot smoothing
# Break the dataset into regions, run regressions on those 
# You can control the number of nebiors in lowess
# baseR's and Gplot's lowess curves are different


library(mosaic)
library(tidyverse)

plot(gasbill ~ month, Utilities, pch = 16, col = "orange")
lines(lowess(Utilities$month,Utilities$gasbill), col = "red")

quad.lm <- lm(gasbill ~ month + I(month^2), Utilities)
summary(quad.lm)
b <- coef(quad.lm)
curve(b[1] + b[2] * x + b[3] * x^2, add = TRUE, col = "skyblue")


ggplot(Utilities, aes(month, gasbill))+
  geom_point()+
  geom_smooth(se = FALSE, color = "red")+
  stat_function(fun = function(x) b[1] + b[2]*X + b[3]*x ^2, color = "skyblue")


# in those other doc, only x values are sued ad neghibors 
# looks like this can be overfitted in an extreme case 

# transformations, lowess, 


```


## Class Activity - Qualitative Variables in Regression (Friday)


```{r}
# Question 1

plot(hp ~ qsec, data=mtcars, col=as.factor(cyl))
lm1 <- lm(hp ~ qsec * as.factor(cyl), data = mtcars)
# the * is short hand to express interaction model
summary(lm1)


# to see the current set palette
palette()
# black, red, green
# to declare a palette
palette(c("skyblue", "firebrick", "orange"))
# the assignment of colors 
# factor is the category and the levels are the values into the category


# ggplot coloring
ggplot(mtcars, aes(qsec, hp, color = as.factor(cyl)))+
  geom_point()+
  scale_color_manual(values = c("skyblue", "firebrick", "orange"), 
                     name = "cyl")+ 
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x)




plot(hp ~ qsec, data=mtcars, col=as.factor(cyl))
b <- coef(lm1)
curve(b[1] + b[2] * qsec + b[3]* cyl6 + b[4] * cyl8 + b[5] * qsec * cyl6 * something)

# Question 2


library(dplyr)
View(starwars)
star.lm <- lm(mass ~ height * species, data=starwars)
summary(star.lm)


ggplot(starwars, aes(height, mass, color = species))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x)




```


```{r}
# Get some data for X and Y
X <- cars$speed
Y <- cars$dist

# Ensure the data has no missing values in any of the X,Y pairs
keep <- which(!is.na(X) & !is.na(Y))
X <- X[keep]
Y <- Y[keep]

# Decide on the fraction of points to use for the local regressions
f <- 1/2

# Identify the total sample size and sample size corresponding to f
n <- length(X) #total sample size
nn <- floor(n*f) #number of dots corresponding to percentage f

# Create storage for the lowess fitted-values
lfit <- rep(NA,n)

# Begin the for loop to compute lowess fitted-values
for (xc in 1:n){ #let each dot be the center dot, one at a time
  xdists <- X - X[xc]  #compute distance from all X-values to center dot x-value
  r <- sort(abs(xdists))[nn] #locate the nn"th" largest x-distance from center x-value
  xdists.nbrhd <- which(abs(xdists) < r) #identify x-values within the neighborhood
  w <- rep(0, length(xdists)) #initialize the weights for all x-values at 0
  w[xdists.nbrhd] <- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3 #tri-cubic weight function 
  plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w), #color dots by their weights
       col=rgb(.2,.2,.2,.3), cex=1.5, yaxt='n', xaxt='n', xlab="", ylab="")
  points(Y[xc] ~ X[xc], pch=16, col="orange") #add center dot to plot
  # lmc <- lm(Y ~ X, weights=w) #run weighted regression
  lmc <- lm(Y ~ X + I(X^2), weights=w) #run weighted regression
  # curve(lmc$coef[1] + lmc$coef[2]*x, #draw line on neighborhood using from and to
  curve(lmc$coef[1] + lmc$coef[2]*x, #draw line on neighborhood using from and to
        from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col="orange", add=TRUE)
  lines(lfit[1:xc] ~ X[1:xc], col="gray") #add lowess line up to current center dot
  
  #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2))
  cat("\n\n")
  readline(prompt=paste0("Center point is point #", xc, "... Press [enter] to continue..."))
  
  
  MADnotThereYet <- TRUE
  count <- 0
  while(MADnotThereYet){ #while loop will continue until "MadnotThereYet" becomes false
    
    readline(prompt=paste0("\n   Adjusting line to account for outliers in the y-direction... Press [enter] to continue..."))   
    
    # overwrite current regression to lighter color if new regression still needed:
    # curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col="wheat", add=TRUE)
    curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col="wheat", add=TRUE)
    
    # update the weights
    MAD <- median(abs(lmc$res))
    resm <- lmc$res/(6*MAD)
    resm[resm>1] <- 1
    bisq <- (1-resm^2)^2
    w <- w*bisq
    obs <- coef(lmc)
    # lmc <- lm(Y ~ X, weights=w)
    lmc <- lm(Y ~ X, weights=w)
    
    curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col="orange", add=TRUE)
    
    #stopping criterion for the weighted regressions in y-direction
    count <- count + 1
    if ( (sum(abs(obs-lmc$coef))<.1) | (count > 3))
      MADnotThereYet <- FALSE
    
  }
  
  curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col="green", add=TRUE)
  points(lmc$coef[1] + lmc$coef[2]*X[xc] ~ X[xc], pch=16, col="green")
  
  
  readline(prompt=paste0("\n   Use final line to get fitted value for this point... Press [enter] to continue to next point..."))
  
  lfit[xc] <- predict(lmc, data.frame(X=X[xc]))
  lines(lfit[1:xc] ~ X[1:xc], col="gray")
  
  
  if (xc == n){
    readline(prompt=paste0("\n  Press [enter] to see actual Lowess curve..."))
    lines(lowess(X,Y, f=f), col="firebrick")
    legend("topleft", bty="n", legend="Actual lowess Curve using lowess(...)", col="firebrick", lty=1)
  }
  
  
}

```




## Skills Quiz: Lowess Curves and Model Selection

### Question 1



### Question 2 



### Question 3
























